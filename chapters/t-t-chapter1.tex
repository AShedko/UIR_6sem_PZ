\chapter{Анализ проблематики задач классификации когнитивных состояний}
\label{chapter1}
\begin{annotation}
	В первой главе подробно рассматриваются теоретические аспекты задачи понижения размерности, задачи классификации (Метод опорных векторов (SVM), нейронные сети, Линейный дискриминантный анализ (ЛДА)) и специфических для проблемной области (фМРТ) подходов к анализу данных. Также описываются программные средства визуализации трёхмерных данных с примерами их использования. (\verb|nilearn.plotting|\cite{10.3389/fninf.2014.00014}, \verb|matplotlib3d|\cite{Hunter:2007}, \verb|NIFTI|, \verb|MITK|\cite{wolf2004medical})
\end{annotation}

\section{Изучение и анализ подходов к классификации когнитивных состояний по данным фМРТ (статическим и динамическим) применительно к задачам медицинской диагностики}
\begin{annotation}
	Для каждого образца объекта или события с известным классом $y$ рассматривается набор наблюдений $x$ (называемых ещё признаками, переменными или измерениями). Набор таких образцов называется обучающей выборкой (или набором обучения, обучением). Задачи классификации состоит в том, чтобы построить хороший прогноз класса $y$ для всякого так же распределённого объекта (не обязательно содержащегося в обучающей выборке), имея только наблюдения $x$.	
\end{annotation}	

В роли объектов выступают пациенты. Признаки характеризуют результаты обследований, симптомы заболевания и применявшиеся методы лечения. Примеры бинарных признаков: пол, наличие головной боли, слабости. Порядковый признак — тяжесть состояния (удовлетворительное, средней тяжести, тяжёлое, крайне тяжёлое). Количественные признаки — возраст, пульс, артериальное давление, содержание гемоглобина в крови, доза препарата. Признаковое описание пациента является, по сути дела, формализованной историей болезни. Накопив достаточное количество прецедентов в электронном виде, можно решать различные задачи:
\begin{compactitem}
	\item классифицировать вид заболевания (дифференциальная диагностика);
	\item определять наиболее целесообразный способ лечения;
	\item предсказывать длительность и исход заболевания;
	\item оценивать риск осложнений;
	\item находить синдромы — наиболее характерные для данного заболевания совокупности симптомов.	
\end{compactitem}
Ценность такого рода систем в том, что они способны мгновенно анализировать и обобщать огромное количество прецедентов — возможность, недоступная специалисту-врачу.


\section{Сравнительный анализ методов классификации многомерных данных }
\begin{annotation}
	Рассмотрим такие методы как: Метод опорных векторов (SVM), Линейный дискриминантный анализ (ЛДА), Логистическая Регрессия
\end{annotation}

Вначале дадим общее определение \textit{задачи классификации (обучения с учителем)}.

Существует неизвестная целевая зависимость — отображение $y^{*}: X\to Y$, значения которой известны только на объектах конечной обучающей выборки $\left\{(x_i,y_i)| i \in \overline{1,P}\right\}$, где $P$--- количество примеров, $x_i \in  X, y_i \in Y$, $X$-- пространство входных признаков, чаще всего действительное векторное пространство ($\mathbb R^k$), $Y$-- конечное множество классов. Часто множество $Y$ является 2 элементным, в этом случае классификация называется \textit{бинарной}. Требуется построить  алгоритм $\alpha: X\to Y$, который для каждого $x \in X$  построить хороший прогноз класса $y$.

Говорят также, что алгоритм должен обладать способностью к обобщению эмпирических фактов, или выводить общее знание (закономерность, зависимость) из частных фактов (наблюдений, прецедентов).

Данная постановка является обобщением классических задач аппроксимации функций. В классической аппроксимации объектами являются действительные числа или векторы. В реальных прикладных задачах входные данные об объектах могут быть неполными, неточными, неоднородными, нечисловыми. Эти особенности приводят к большому разнообразию методов обучения с учителем. Далее приводятся описания некоторых методов бинарной классификации данных. Более подробные описания и обобщения на случай многоклассовой классификации приводятся на веб-сайте \url{machinelearning.ru} \cite{mlru}

\subsection{Логистическая регрессия}
Пусть объекты описываются n числовыми признаками  $f_j:\: X\to\mathbb{R},\; j=1,\ldots,n$. Тогда пространство признаковых описаний объектов есть $X=\mathbb{R}^n$. Пусть $Y$ — конечное множество номеров (имён, меток) классов.
Пусть задана обучающая выборка пар «объект, ответ»  $X^m = \{(x_1,y_1),\dots,(x_m,y_m)\}$.
Случай двух классов
Положим $Y=\{-1,+1\}$. В логистической регрессии строится линейный алгоритм классификации $a:\; X\to Y$ вида
$a(x,w) = \mathrm{sign}\left( \sum_{j=1}^n w_j f_j(x) - w_0 \right) = \mathrm{sign}\langle x,w \rangle$,
где  $w_j$ — вес $j$-го признака,  $w_0$ — порог принятия решения, $w=(w_0,w_1,\ldots,w_n)$ — вектор весов, $\langle x,w \rangle$ — скалярное произведение признакового описания объекта на вектор весов. Предполагается, что искусственно введён «константный» нулевой признак: $f_{0}(x)=-1.$
Задача обучения линейного классификатора заключается в том, чтобы по выборке  $X^m$ настроить вектор весов $w$. В логистической регрессии для этого решается задача минимизации эмпирического риска с функцией потерь специального вида:
\begin{equation}
Q(w) = \sum_{i=1}^m \ln\left( 1 + \exp( -y_i \langle x_i,w \rangle ) \right) \to \min_{w}.
\end{equation}
После того, как решение w найдено, становится возможным не только вычислять классификацию a(x) = $\mathrm{sign}\langle x,w \rangle$ для произвольного объекта x, но и оценивать апостериорные вероятности его принадлежности классам:
\begin{equation}
\mathbb{P}\{y|x\} = \sigma\left( y \langle x,w \rangle\right),\;\; y\in Y,
\end{equation}
где $\sigma(z) = \frac1{1+e^{-z}}$ — сигмоидная функция. Во многих приложениях апостериорные вероятности необходимы для оценивания рисков, связанных с возможными ошибками классификации.

\subsection{ЛДА}
\textbf{Линейный дискриминантный анализ} (ЛДА), а также связанный с ним \textit{линейный дискриминант Фишера} — методы статистики и машинного обучения, применяемые для нахождения линейных комбинаций признаков, наилучшим образом разделяющих два или более класса объектов или событий. Полученная комбинация может быть использована в качестве линейного классификатора или для сокращения размерности пространства признаков перед последующей классификацией.

Рассмотрим этот метод для случая 2 классов:

При ЛДА предполагается, что функции совместной плотности распределения вероятностей $p(\vec{x}|y=1)$ и $p(\vec{x}|y=0)$ - нормальны. В этих предположениях оптимальное байесовское решение~-- относить точки ко второму классу если отношение правдоподобия ниже некоторого порогового значения $T$: 
$$(\vec{x}-\vec{\mu}_0)^T\Sigma_{y=0}^{-1}(\vec{x}-\vec{\mu}_0)+\ln{|\Sigma _{y=0}|}-(\vec{x}-\vec{\mu}_1)^T\Sigma _{y=1}^{-1}(\vec{x}-\vec{\mu}_1)-\ln{|\Sigma_{y=0}|}<T$$
Если не делается никаких дальнейших предположений, полученную задачу классификации называют квадратичным дискриминантным анализом (\textit{англ. quadratic discriminant analysis, QDA}). В ЛДА делается дополнительное предположение о \textit{гомоскедастичности} (т.е. предполагается, что ковариационные матрицы равны, $\Sigma_{y=0}=\Sigma_{y=1}=\Sigma$) и считается, что ковариационные матрицы имеют полный ранг. При этих предположениях задача упрощается и сводится к сравнению скалярного произведения с пороговым значением 
$$\vec{\omega}\cdot\vec{x}<c $$
\noindent
для некоторой константы $c$, где 
$$\vec{\omega}=\Sigma^{-1}(\vec{\mu_1}-\vec{\mu_0}). $$
\noindent
Это означает, что вероятность принадлежности нового наблюдения x к классу y зависит исключительно от линейной комбинации известных наблюдений.


\subsection{SVM}
Что предпринимать, если данные не гомоскедастичны?
Рассмотрим метод опорных векторов, для чего вначале дадим определение метода.

\textbf{Метод опорных векторов} (\emph {англ. SVM, support vector machine}) — набор схожих алгоритмов обучения с учителем, использующихся для задач классификации и регрессионного анализа. SVM в чистом виде ~-- линейный классификатор.

Основная идея метода — перевод исходных векторов в пространство более высокой размерности и поиск разделяющей гиперплоскости с максимальным зазором в этом пространстве. Две параллельных гиперплоскости строятся по обеим сторонам гиперплоскости, разделяющей классы. Разделяющей гиперплоскостью будет гиперплоскость, максимизирующая расстояние до двух параллельных гиперплоскостей. Алгоритм работает в предположении, что чем больше разница или расстояние между этими параллельными гиперплоскостями, тем меньше будет средняя ошибка классификатора.


Рассмотрим задачу нахождения наилучшего в некотором смысле разделения множества векторов на два класса с помощью линейной решающей функции. Пусть имеется множество прецедентов $(\Xi ,Y)$, где $\Xi  = \{ {\vec{x}}_1 ,...,{\vec{x}}_N \}$ — обучающая выборка, а $Y = (y_1 ,...,y_N )$ — множество меток двух классов $\omega _1$ и $\omega _2$. Требуется по обучающей выборке построить линейную решающую функции, т.е. такую линейную функцию $f({\vec{x}})$, которая удовлетворяла бы условию

\[f({\vec{x}}_i ) > 0 для всех {\vec{x}}_i  \in \omega _1,\\
f({\vec{x}}_i ) < 0 для всех {\vec{x}}_i  \in \omega _2.\]

Без ограничения общности можно считать, что метки классов равны
$$y_i  = \left\{\:\;1,\;{\vec{x}}_i  \in \varpi _1 , \\- 1,\;{\vec{x}}_i  \in \varpi _2 . \right.$$                                       
Тогда поставленную выше задачу можно переформулировать следующим образом. Требуется найти линейную решающую функцию $f({\vec{x}})$, которая бы удовлетворяла условию
\begin{equation}\label{e1}
y_i f({\vec{x}}_i ) > 0  \text{ для всех } {\vec{x}}_i  \in \Xi
\end{equation}
Умножая, если нужно, функцию $f$ на некоторое положительное число, нетрудно видеть, что система неравенств (\ref{e1}) равносильна системе
$$y_i f({\vec{x}}_i ) > 1 для всех  {\vec{x}}_i  \in \Xi$$
Кроме того, так как $f({\vec{x}})$ — линейная функция, то последняя система неравенств примет вид
(\ref{e2})
\begin{equation}\label{e2}
y_i (({\vec{w}},{\vec{x}}_i ) + b) \ge 1,\quad i = 1,...,N,
\end{equation}
где ${\vec{w}}$ — вектор весовых коэффициентов, $b$ — некоторое число. Тогда разделяющей два класса гиперплоскостью будет $({\vec{w}},{\vec{x}}) + b = 0$\,. Нетрудно видеть, что и все гиперплоскости вида $({\vec{w}},{\vec{x}}) + b' = 0$, где $b' \in (b - 1,b + 1)$, также будут разделяющими.
Расстояние между граничными гиперплоскостями $({\vec{w}},{\vec{x}}) + b - 1 = 0$ и $({\vec{w}},{\vec{x}}) + b + 1 = 0$ равно $\frac {{2}}{{\left\| {\vec{w}} \right\|}}$ .
Действительно, $\left( {\frac{{\vec{w}}}{{\left\| {\vec{w}} \right\|}},{\vec{x}}} \right) + \frac{{b - 1}}{{\left\| {\vec{w}} \right\|}} = 0$ и $\left( {\frac{{\vec{w}}}{{\left\| {\vec{w}} \right\|}},{\vec{x}}} \right) + \frac{{b + 1}}{{\left\| {\vec{w}} \right\|}} = 0$ — нормальные уравнения этих гиперплоскостей.
Тогда $p_1  = \frac{{b - 1}}{{\left\| {\vec{w}} \right\|}} и p_2  = \frac{{b + 1}}{{\left\| {\vec{w}} \right\|}}$ — расстояния от этих гиперплоскостей до начала координат и
$\frac {{2}}{{\left\| {\vec{w}} \right\|}}$  — расстояние между гиперплоскостями. На самих граничных плоскостях может находиться некоторое число обучающих векторов. Эти векторы называются опорными.

Для надежного разделения классов необходимо, чтобы расстояние между разделяющими гиперплоскостями было как можно большим, т.е. $\left\| {\vec{w}} \right\|$ была как можно меньше. Таким образом, ставится задача нахождения минимума квадратичного функционала $0.5({\vec{w}},{\vec{w}})$ (коэффициент $0.5$ вводится для удобства дифференцирования) в выпуклом многограннике, задаваемым системой неравенств (2). В выпуклом множестве квадратичный функционал всегда имеет единственный минимум (если это множество не пусто). Из теоремы Куна — Таккера следует, что решение этой оптимизационной задачи равносильно поиску седловой точки лагранжиана
$$\mathcal L({\vec{w}},b,{\vec{\lambda }}) = 0.5({\vec{w}},{\vec{w}}) - \sum\limits_{i = 1}^N {\lambda _i (y_i (({\vec{w}},{\vec{x}}_i ) + b) - 1)}  \to \ \min \limits_{{\vec{w}},b} \ \max\limits_{\vec{\lambda }}$$

в ортанте по множителям Лагранжа $\lambda _i  \geqslant 0\;\;(i = 1,...,N)$, при условии, что
$\lambda _i \left( {y_i (({\vec{w}},{\vec{x}}_i ) + b) - 1} \right) = 0,\quad i = 1,...,N$.
Последнее условие равносильно тому, что
\begin{equation}
\lambda _i  = 0  или  y_i (({\vec{w}},{\vec{x}}_i ) + b) - 1 = 0,\quad i = 1,...,N
\end{equation}
Из необходимых условий существования седловой точки (полагая ${\vec{x}}_i  = (x_{i1} ,x_{i2} ,...,x_{in} )$) имеем
\begin{empheq}[left = \empheqlbrace]{align*}
	0 = \frac{{\partial L}}{{\partial w_j }} &= w_j  - \sum\limits_{i = 1}^N {\lambda _i y_i x_{ij} } ,\quad j = 1,...,n, \\
0 = \frac{{\partial L}}{{\partial b}} &= \sum\limits_{i = 1}^N {\lambda _i y_i } .
\end{empheq}
Откуда следует, что вектор ${\vec{w}}$ следует искать в виде
\begin{equation}
{\vec{w}} = \sum\limits_{i = 1}^N {\lambda _i y_i {\vec{x}}_i },
\end{equation}
причем
\begin{equation}
\sum\limits_{i = 1}^N {\lambda _i y_i }  = 0.
\end{equation}
В силу (3) в сумму (4) с ненулевыми коэффициентами $\lambda_i$ входят только те векторы, для которых $y_i (({\vec{w}},{\vec{x}}_i ) + b) - 1 = 0$. Такие векторы называют опорными, так как это именно те векторы, через которые будут проходить граничные гиперплоскости, разделяющие классы. Для найденного весового вектора $\vec{w}$ смещение b можно вычислить как $b = {y_s}^{-1} - ({\vec{w}},{\vec{x}}_s )$ для любого опорного вектора ${\vec{x}}_s$ .
Найдем значения множителей Лагранжа, как критических точек лагранжиана. Для этого подставим (4) и (5) в лагранжиан, получим
\[\mathcal{L}({\vec{w}},b,{\vec{\lambda }}) = 0.5({\vec{w}},{\vec{w}}) - \sum\limits_{i = 1}^N {\lambda _i (y_i (({\vec{w}},{\vec{x}}_i ) + b) - 1)}  = \\
0.5({\vec{w}},{\vec{w}}) - \left( {({\vec{w}},{\vec{w}}) - \sum\limits_{i = 1}^N {\lambda _i } } \right) = \sum\limits_{i = 1}^N {\lambda _i }  - 0.5({\vec{w}},{\vec{w}}) = \\
\sum\limits_{i = 1}^N {\lambda _i }  - 0.5\sum\limits_{i,j = 1}^N {\lambda _i \lambda _j y_i y_j ({\vec{x}}_i ,{\vec{x}}_j )}  = \sum\limits_{i = 1}^N {\lambda _i }  - 0.5\left\| {\sum\limits_{i = 1}^N {\lambda _i y_i {\vec{x}}_i } } \right\|^2 .
\]
Таким образом, задача сводится к нахождению критических точек функции
\begin{equation}
\Phi ({\vec{\lambda }}) = \sum\limits_{i = 1}^N {\lambda _i }  - 0.5\left\| {\sum\limits_{i = 1}^N {\lambda _i y_i {\vec{x}}_i } } \right\|^2 .
\end{equation}
Так как эта функция представляет собой разность линейной и квадратичной функций, причем квадратичная функция отрицательно определена, то требуется найти наибольшее значение функции $\Phi ({\vec{\lambda }})$ при условии $\sum\nolimits_{i = 1}^N {\lambda _i y_i }  = 0$ в области $\lambda _i  \ge 0\;\;(i = 1,...,N)$.  В теории оптимизации существует множество алгоритмов решения этой задачи (например, градиентные методы, метод покоординатного спуска и т.д.).

В 1992 году в работе Бернарда Бозера (Boser B.), Изабелл Гийон (Guyon I.) и Владимира Вапника был предложен способ адаптации машины опорных векторов для \emph{нелинейного разделения классов}.

\section{Сравнительный анализ программных средств анализа и визуализации трехмерных данных фМРТ и исследование возможности их использования}

\begin{annotation}
	В разделе описаны  различные программные компоненты для визуализации нейро-данных.
\end{annotation}

\subsection*{Nilearn}
Данная библиотека предоставляет с лёгкостью использовать продвинутые техники машинного обучения, распознавания образов и статистики на <<нейроданных>> для таких задач как MVPA (многовоксельный анализ закономерностей, \textit{англ. Mutli-Voxel Pattern Analysis}), декодирование, предиктивное моделирование и других.

\texttt{Nilearn} может быть использован для анализа данных фМРТ в состоянии покоя и в случае выполнения испытуемым задач. Данная библиотека создана на основе библиотеки \texttt{SciKit-Learn} для языка \texttt{python} в которой уже реализована значительная часть алгоритмов описанных выше.

\subsection*{Analyze}
\texttt{Analyze}~-- ППП, разработанный в \textit{Mayo Clinic} компанией Biomedical Imaging Resource (BIR) для многомерных отображения, обработки и измерения медицинских изображений различного типа. Это коммерческая программа, импользуемая для изучения томорамм, результатов фМРТ, компьютерной томографии, позитрон-эмиссионной томографии (PET).

\subsection*{MITK}
\texttt{Medical Imaging Interaction Toolkit (MITK)}-- свободная система с открытым исходным кодом для разработки интерактивного
ПО для обработки медицинских изображений. Внутри себя, MITK содержит Insight Toolkit (ITK), Visualization Toolkit (VTK) и набор инструментов для разработки приложений. Разработана в \textit{German Cancer Research Center
Division of Medical and Biological Informatics}

\subsection{SPM12}


\section{Выводы и постановка задачи курсового проекта}

Это всегда последний пункт. Здесь, по-первых, приводятся, попунктно, основные вывода из проделанного анализа. Например:

\begin{compactenum}
	\item Выполнен сравнительный анализ таких-то формальных систем с точки зрения применимости к решению  задачи классификации. Из-за доступности и легкости их применения решено провести сравнение их успешности для этой задачи
	\item Были проанализированы варианты программных архитектур на основе систем. С учетом требований к поддержке больших объемов данных и высоких требований к потенциалу модернизируемости, была выбрана за основу такая-то архитектура.
	\item Сравнительный анализ таких-то библиотек показал, что библиотека X проще в использовании, но менее производительна, в то время как библиотека Y обеспечивает высокую производительность, но и требует значительных трудозатрат для использования. В связи с такими-то соображениями были принято решение использовать такую-то библиотеку.
\end{compactenum}

